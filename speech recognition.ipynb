{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [00:09, 1052.11 docs/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import threading    # Import the threading module to run the speech recognition in a separate thread\n",
    "import pyttsx3  # Import the pyttsx3 library to convert text to speech\n",
    "import speech_recognition as sr    # Import the speech recognition library\n",
    "import openai  # Import OpenAI API\n",
    "from haystack.document_stores import InMemoryDocumentStore  # Import InMemoryDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from document import documents  # Import documents from the external file\n",
    "\n",
    "#my openAI private secret key\n",
    "openai.api_key = \"you open AI key\"\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 150)\n",
    "tts_engine.setProperty('volume', 1)\n",
    "\n",
    "# Initialize speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Initialize Haystack's RAG pipeline\n",
    "document_store = InMemoryDocumentStore(embedding_dim=768)\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=False\n",
    ")\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")\n",
    "rag_pipeline = ExtractiveQAPipeline(reader, retriever)\n",
    "\n",
    "# writing and updating from document.py file\n",
    "document_store.write_documents(documents)\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "print(\"Components initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_to_user():\n",
    "    # Listen to the user's voice input and convert it to text.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)  # Listen for 5 seconds\n",
    "            user_input = recognizer.recognize_google(audio)\n",
    "            print(f\"You said: {user_input}\")\n",
    "            return user_input\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I didn't catch that. Please try again.\")\n",
    "            return None\n",
    "        except sr.RequestError:\n",
    "            print(\"Network error. Please check your connection.\")\n",
    "            return None\n",
    "\n",
    "def generate_response_with_rag(query):\n",
    "    # Generate a response using Haystack's RAG model.\n",
    "    try:\n",
    "        print(\"Searching for relevant information...\")\n",
    "        result = rag_pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 1}, \"Reader\": {\"top_k\": 1}})  # Run the RAG pipeline\n",
    "        if result[\"answers\"]:\n",
    "            return result[\"answers\"][0].answer\n",
    "        else:\n",
    "            return \"I couldn't find any relevant information in my database.\"\n",
    "    except Exception as e:  # Catch any errors that occur during the search\n",
    "        print(f\"Error using RAG: {e}\")\n",
    "        return \"I encountered an error while searching for an answer.\"\n",
    "\n",
    "def generate_response_with_gpt(prompt):\n",
    "    # Generate a response using OpenAI's GPT-3.5 model. as it is better in scale for perfoamce\n",
    "    # openAI gpt models perform better than llm so we decided to use the openAI gpt model\n",
    "    try:\n",
    "        print(\"Generating response using GPT-3.5...\")\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip()  # Return the generated response\n",
    "    except Exception as e:\n",
    "        print(f\"Error using GPT-3.5: {e}\")\n",
    "        return \"I'm sorry, I couldn't generate a response at this time.\"\n",
    "\n",
    "def speak_output(response):\n",
    "    # Speak the response using text-to-speech.\n",
    "    # we have used threads to run the speech recognition in a separate thread\n",
    "    try:\n",
    "        tts_engine.stop()  # Interrupt any ongoing speech\n",
    "        tts_engine.say(response)  # Speak the response\n",
    "        tts_engine.runAndWait()  # Wait for the speech to finish\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error with text-to-speech: {e}\")\n",
    "\n",
    "def stop_speech():\n",
    "    # Stop the text-to-speech engine.\n",
    "    try:\n",
    "        tts_engine.stop()  # Stop the speech\n",
    "    except RuntimeError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #this main function completes the whole process of the chatbot\n",
    "    print(\"Bot is ready. You can speak now!\")\n",
    "    speak_output(\"Hello! I am your assistant. How can I help you today?\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = listen_to_user()\n",
    "        if user_input is None:\n",
    "            continue\n",
    "\n",
    "        # to exit the ongoing conversation user can say \"exit\" or \"quit\"\n",
    "        if \"exit\" in user_input.lower() or \"quit\" in user_input.lower():\n",
    "            print(\"Goodbye!\")\n",
    "            speak_output(\"Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Generate response using RAG\n",
    "        #documents store all the rag information for faster retrival\n",
    "        response = generate_response_with_rag(user_input)\n",
    "\n",
    "        # If RAG fails to find relevant content, model fallbacks to GPT to fetch answer\n",
    "        if response == \"I couldn't find any relevant information in my database.\":\n",
    "            response = generate_response_with_gpt(user_input)\n",
    "\n",
    "        # Print and speak the bot's response\n",
    "        print(f\"Bot: {response}\")\n",
    "        speak_output(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is ready. You can speak now!\n",
      "Listening...\n",
      "You said: what is AI\n",
      "Searching for relevant information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.21s/ Batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines\n",
      "Listening...\n",
      "You said: what is life\n",
      "Searching for relevant information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: existence\n",
      "Listening...\n",
      "You said: what are you doing\n",
      "Searching for relevant information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: to help you.\n",
      "Listening...\n",
      "You said: exit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start the chatbot\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
